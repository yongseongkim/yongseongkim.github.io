<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">AlphaGo Paper Review | yongseongkim&#x27;s blog</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://yongseongkim.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://yongseongkim.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://yongseongkim.github.io/blog/2019/01/19/alphago-lee/"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="AlphaGo Paper Review | yongseongkim&#x27;s blog"><meta data-rh="true" name="description" content="최근 David Silver 교수의 Reinforcement Learning Course 를 보면서 강화학습에 대해 공부하다 연장선상으로 뒤늦게 AlphaGo 논문을 읽었습니다."><meta data-rh="true" property="og:description" content="최근 David Silver 교수의 Reinforcement Learning Course 를 보면서 강화학습에 대해 공부하다 연장선상으로 뒤늦게 AlphaGo 논문을 읽었습니다."><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2019-01-19T00:00:00.000Z"><meta data-rh="true" property="article:tag" content="reinforcement learning,alphago"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://yongseongkim.github.io/blog/2019/01/19/alphago-lee/"><link data-rh="true" rel="alternate" href="https://yongseongkim.github.io/blog/2019/01/19/alphago-lee/" hreflang="en"><link data-rh="true" rel="alternate" href="https://yongseongkim.github.io/blog/2019/01/19/alphago-lee/" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="yongseongkim&#39;s blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="yongseongkim&#39;s blog Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.068f18cf.css">
<link rel="preload" href="/assets/js/runtime~main.c5770a4c.js" as="script">
<link rel="preload" href="/assets/js/main.f9b9ab4d.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="yongseongkim&#x27;s blog" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/logo.svg" alt="yongseongkim&#x27;s blog" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">yongseongkim&#x27;s blog</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog/">Blog</a><a class="navbar__item navbar__link" href="/blog/tags/">Tags</a></div><div class="navbar__items navbar__items--right"><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2022/03/10/game-synchronizations/">Synchronization Techniques in Game Development</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2021/09/25/uilabel-alignment/">UILabel Alignment</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2021/06/25/how-to-open-app-by-url/">How To Open An App By URL</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2020/06/14/swift-memory-management/">Memory Management in Swift</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2020/05/26/swiftui-data-flow-patterns/">SwiftUI - Data Flow Patterns</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2020/05/25/swiftui-layout-system/">SwiftUI - Layout System</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2020/02/09/introduction-to-steganography/">Introduction to Steganography</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2019/02/02/alphago-zero/">AlphaGo Paper Review (2)</a></li><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/blog/2019/01/19/alphago-lee/">AlphaGo Paper Review</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h1 class="title_f1Hy" itemprop="headline">AlphaGo Paper Review</h1><div class="container_mt6G margin-vert--md"><time datetime="2019-01-19T00:00:00.000Z" itemprop="datePublished">January 19, 2019</time> · <!-- -->14 min read</div></header><div id="__blog-post-container" class="markdown" itemprop="articleBody"><p>최근 David Silver 교수의 <a href="https://youtu.be/2pWv7GOvuf0" target="_blank" rel="noopener noreferrer">Reinforcement Learning Course</a> 를 보면서 강화학습에 대해 공부하다 연장선상으로 뒤늦게 AlphaGo 논문을 읽었습니다.
바둑에 대해 전혀 모르더라도 AlphaGo 가 어떻게 동작하는지는 이해할 수 있었습니다.
벌써 2-3년이 지난 논문이지만 간단하게 정리해봤습니다.</p><h1>AlphaGo Lee</h1><p>AlphaGo Lee 는 2016년 3/9 부터 3/15 까지 프로바둑 기사 이세돌과 바둑 게임을 한 딥러닝 모델입니다.
이세돌과 경기를 하기 전에는 유럽 바둑대회를 우승했던 Fan Hui 와 붙어 이겼습니다.
AlphaGo 는 크게 AlphaGo Lee / AlphaGo Zero 버전으로 나뉘는데 여기서는 AlphaGo Lee 만 정리하려 합니다.</p><p>바둑은 두 플레이어가 각각 흑백 돌을 두면서 많은 진영을 가진 자가 이기는 게임입니다.
돌을 두기까지 제한된 시간을 쓸 수 있는데 AlphaGo 는 이 시간 동안 트리를 이용하여 경우의 수를 탐색한 후 가장 좋은 수를 둡니다.
이미 컴퓨터로 정복된 체스는 경우의 수가 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><msup><mn>0</mn><mn>120</mn></msup></mrow><annotation encoding="application/x-tex">10^{120}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">120</span></span></span></span></span></span></span></span></span></span></span></span></span> 입니다.
하지만 바둑의 경우의 수는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><msup><mn>0</mn><mn>170</mn></msup></mrow><annotation encoding="application/x-tex">10^{170}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">170</span></span></span></span></span></span></span></span></span></span></span></span></span> 으로 모든 경우의 수를 계산하려면 컴퓨터로 수십억 년이 걸린다고 합니다.
AlphaGo 는 이 많은 경우의 수를 어떻게 처리했을까요?</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="가능성이-있는-수들만-탐색해본다">가능성이 있는 수들만 탐색해본다.<a href="#가능성이-있는-수들만-탐색해본다" class="hash-link" aria-label="Direct link to 가능성이 있는 수들만 탐색해본다." title="Direct link to 가능성이 있는 수들만 탐색해본다.">​</a></h3><p>이기기 위해 굳이 엉뚱한 곳에 수를 두고 경우의 수를 탐색할 필요는 없습니다.
승산이 있어 보이는 곳에 수를 두고 그 이후 경우의 수를 탐색해보면 됩니다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="승산이-없는-게임은-굳이-더-탐색하지-않는다">승산이 없는 게임은 굳이 더 탐색하지 않는다.<a href="#승산이-없는-게임은-굳이-더-탐색하지-않는다" class="hash-link" aria-label="Direct link to 승산이 없는 게임은 굳이 더 탐색하지 않는다." title="Direct link to 승산이 없는 게임은 굳이 더 탐색하지 않는다.">​</a></h3><p>흔히 우리는 게임을 볼 때 어떤 상황에서 누가 이겼는지 졌는지를 판단하곤 합니다.
게임의 진행 정도가 지나면 지날수록 초보들도 누가 이겼는지 졌는지를 판단할 수 있습니다.
이처럼 이미 승패가 어느 정도 기운 상태에서는 굳이 그 이후의 수까지 탐색할 필요가 없습니다.</p><p>그러면 승산이 있을 만한 곳이 어디인지, 바둑판만 보고 내가 승산이 있는지 없는지는 어떻게 파악할까요?</p><h1>필요한 Networks</h1><p>이전에 언급했던 것과 같이 터무니없는 수를 두지 않기 위해, 바둑판을 보고 승산이 있는지 없는지를 알기 위해서는 몇 개의 준비물이 필요합니다.</p><ol><li>SL policy network</li><li>Rollout policy network</li><li>RL policy network</li><li>Value network</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="sl-policy-network">SL Policy Network<a href="#sl-policy-network" class="hash-link" aria-label="Direct link to SL Policy Network" title="Direct link to SL Policy Network">​</a></h3><p>Supervised Learning Policy Network 는 바둑판의 상태를 입력으로 다음 수가 어디에 놓일지 예측하는 Network 입니다.
다음 수를 어느 곳에 둘지 나타내는 확률 분포가 출력으로 나옵니다.</p><p><img loading="lazy" alt="distribution" src="/assets/images/next-step-distribution-753409a0280373e5d3d876e22b4cbc2b.png" width="450" height="440" class="img_ev3q"></p><p>출처: <a href="https://www.nature.com/articles/nature16961" target="_blank" rel="noopener noreferrer">Mastering the game of Go with deep neural networks and tree search</a></p><p>전문가들의 기보를 기반으로 트레이닝하여 다음 수를 예측하는 데 대략 57% 의 정확도를 보였다고 합니다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="rollout-policy-network">Rollout Policy Network<a href="#rollout-policy-network" class="hash-link" aria-label="Direct link to Rollout Policy Network" title="Direct link to Rollout Policy Network">​</a></h3><p>위 SL Policy Network 와 같이 전문가의 기보를 학습시킵니다. 출력도 다음 수에 관한 확률 분포로 위의 Policy Network 와 거의 비슷합니다.
하지만 입력에 대해 CNN을 사용하지 않고 사람이 생각하는 특징들을 입력으로 받으며 상대적으로 SL Policy Network 보다 가볍게 만든 Network 입니다.
정확도는 24% 정도로 SL Policy Network 보다 많이 낮습니다. 하지만 처리 시간은 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>μ</mi><mi>s</mi></mrow><annotation encoding="application/x-tex">2{\mu}s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em"></span><span class="mord">2</span><span class="mord"><span class="mord mathnormal">μ</span></span><span class="mord mathnormal">s</span></span></span></span></span> 로 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mi>m</mi><mi>s</mi></mrow><annotation encoding="application/x-tex">3ms</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">3</span><span class="mord mathnormal">m</span><span class="mord mathnormal">s</span></span></span></span></span> 걸리는 SL Policy Network 보다 1,000배가 넘게 빠릅니다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="rl-policy-network">RL Policy Network<a href="#rl-policy-network" class="hash-link" aria-label="Direct link to RL Policy Network" title="Direct link to RL Policy Network">​</a></h3><p>Reinforcement Learning Policy Network 로 Network 끼리 플레이를 하면서 Game 을 이기는 방향으로 성장합니다.
Network 구조는 SL Policy Network 와 같습니다. 초기 사이사이 Weight 도 SL Policy Network 와 같은 값으로 초기화합니다.
경쟁 상대는 학습시켜오던 과정 중에 있던 Network 들을 모아 임의로 추출합니다.
예를 들면 SL Policy Network, 500번 학습한 RL Policy Network, 1000번 학습한 RL Policy Network ... 와 같이 Opponent Pool 에 저장한 후 이 Pool 에서 임의로 선택한 Network 와 시합을 하며 성장합니다.
Overfitting 을 막기 위해 많이 학습한 RL Policy Network 끼리 학습시키지 않고 다양한 적들과 상대합니다.
나중에 결과물의 RL Policy Network 는 SL Policy Network 를 상대로 80% 승률을 보였다고 합니다.
정교한 MCTS 를 이용한 프로그램 Pachi 를 상대로는 85% 승률을 보였다고 합니다.
SL Policy Network 는 Pachi 를 상대로 11% 승률을 보였다고 합니다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="value-network">Value Network<a href="#value-network" class="hash-link" aria-label="Direct link to Value Network" title="Direct link to Value Network">​</a></h3><p>Value Network 는 바둑판의 상태를 보고 승률을 예측합니다.
위에서 설명한 Policy Network 들은 바둑판의 상태를 보고 다음 수에 대한 확률 분포를 내뱉습니다.
하지만 Value Network 는 같은 구조로 구성되지만, 승률만 내뱉습니다.</p><p><img loading="lazy" alt="Policy Network, Value network 의 Output" src="/assets/images/policy-value-network-output-636bfdadf8954c241d11f11b880500aa.png" width="800" height="760" class="img_ev3q"></p><p>출처: <a href="https://www.nature.com/articles/nature16961" target="_blank" rel="noopener noreferrer">Mastering the game of Go with deep neural networks and tree search</a></p><p>KGS 데이터로 학습시킨 Value Network 는 일반적으로 승률을 계산하기보다는 그전 결과들을 기억하여 트레이닝 데이터에서는 MSE 가 0.19이었지만 테스트 데이터에서는 0.37이 나왔습니다.
이러한 Overfitting 문제를 피하고자 RL Policy Network 끼리 경기한 3,000만 플레이 데이터를 모았다고 합니다.</p><p><img loading="lazy" alt="Value Network 성능 비교" src="/assets/images/value-network-mse-compare-8eb6e5d26374d080c2f473d73617941f.png" width="1000" height="480" class="img_ev3q"></p><p>출처: <a href="https://www.nature.com/articles/nature16961" target="_blank" rel="noopener noreferrer">Mastering the game of Go with deep neural networks and tree search</a></p><p>위에 표에서 X 축 오른쪽으로 갈수록 경기가 많이 진행됐음을 의미한다. 경기가 진행되면 될수록 초보자도 승패를 알 수 있으니 대체적으로 error 가 낮아집니다.
하지만 여기서 Policy Network 로 예측한 승률과 Value Network 의 승률 계산이 크게 차이 나지 않음을 볼 수 있습니다.
Policy Network 가 수를 두면서 결괏값을 예측하는 것보다 바둑판의 상태를 보고 승률을 예측하는 Value Network 가 매우 빠르므로 Value Network 를 사용합니다.</p><h1>MCTS: Monte Carlo Tree Search</h1><p>Monte Carlo 는 계산하려는 값을 구하기 힘들 때 수많은 시도를 통해 근사적으로 계산하는 방법을 말한다.
아래 그림은 수많은 점을 찍어 원주율을 계산하는 과정입니다.</p><p><img loading="lazy" alt="Monte Carlo Example" src="/assets/images/Pi_30K-e925f899ac858bd1df31047eec8ea00d.gif" width="500" height="500" class="img_ev3q"></p><p>출처: <a href="https://ko.wikipedia.org/wiki/%EB%AA%AC%ED%85%8C%EC%B9%B4%EB%A5%BC%EB%A1%9C_%EB%B0%A9%EB%B2%95" target="_blank" rel="noopener noreferrer">위키피디아 - 몬테카를로 방법</a></p><p>MCTS 는 가상으로 수많은 수를 두면서 트리를 구성하는 방식이다. 많은 게임을 하면 할수록 트리는 더 다양한 상황을 탐색하고 더 좋은 수를 알아낼 수 있습니다.
MCTS 는 크게 4 step(Selection, Expansion, Evaluation, Backup) 으로 나눌 수 있습니다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="selection-expansion">Selection, Expansion<a href="#selection-expansion" class="hash-link" aria-label="Direct link to Selection, Expansion" title="Direct link to Selection, Expansion">​</a></h3><p>MCTS 의 첫 과정 Selection, Expansion 은 SL Policy 를 이용하여 트리를 만들어나갑니다.
트리에는 상태 s 에서 행동 a 를 취했을 때 얼마만큼의 가치가 있는지를 나타내는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Q(s, a)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">a</span><span class="mclose">)</span></span></span></span></span>, 방문 횟수 N(s, a) 그리고 사전확률 P(s, a) 등을 저장합니다.</p><p><img loading="lazy" alt="MCTS Selection" src="/assets/images/mcts-step-selection-37ddc2b3c889025ee28dd9d724d22b2c.png" width="411" height="412" class="img_ev3q"></p><p>출처: <a href="https://www.nature.com/articles/nature16961" target="_blank" rel="noopener noreferrer">Mastering the game of Go with deep neural networks and tree search</a></p><p>먼저 위 그림과 같이 루트에서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo>+</mo><mi>u</mi><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Q+u(p)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em"></span><span class="mord mathnormal">Q</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">u</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mclose">)</span></span></span></span></span> 큰 값의 상태를 따라 내려갑니다.</p><p><img loading="lazy" alt="MCTS Expansion" src="/assets/images/mcts-step-expansion-2d966d87817724f97f75d7ce791d85f7.png" width="317" height="433" class="img_ev3q"></p><p>출처: <a href="https://www.nature.com/articles/nature16961" target="_blank" rel="noopener noreferrer">Mastering the game of Go with deep neural networks and tree search</a></p><p>값이 높은 방향으로 선택하면서 내려가다가 이미 방문했던 상태 이외에 큰 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo>+</mo><mi>u</mi><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Q+u(p)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em"></span><span class="mord mathnormal">Q</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">u</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mclose">)</span></span></span></span></span> 를 만나면 그림처럼 가지를 뻗어 나갑니다.
<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">u(p)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">u</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mclose">)</span></span></span></span></span> 는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mi>N</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(s, a) / N(s, a)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">a</span><span class="mclose">)</span><span class="mord">/</span><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">a</span><span class="mclose">)</span></span></span></span></span> 값에 비례합니다.
초기에는 exploration 관점에서 낮은 방문 횟수의 상태를 선호하지만, 점근적으로 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em"></span><span class="mord mathnormal">Q</span></span></span></span></span>가 큰 값을 선택합니다.
방문을 많이 하면 할수록 시뮬레이션을 많이 하고 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em"></span><span class="mord mathnormal">Q</span></span></span></span></span> 값은 더 정교해집니다.
여기서 RL Policy Network 를 쓰지 않는 이유는 SL Policy Network 가 성능이 더 좋았다고 합니다.</p><p><code>Humans select a diverse beam of promising moves, whereas RL potimizes for the single best move</code></p><p>출처: <a href="https://www.nature.com/articles/nature16961" target="_blank" rel="noopener noreferrer">Mastering the game of Go with deep neural networks and tree search</a></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="evaluation">Evaluation<a href="#evaluation" class="hash-link" aria-label="Direct link to Evaluation" title="Direct link to Evaluation">​</a></h2><p>Selection, Expansion 으로 바둑돌을 놓은 후 얼마나 좋은 수인지 가치를 매깁니다.</p><p><img loading="lazy" alt="MCTS Evaluation" src="/assets/images/mcts-step-evaluation-3c7c685eb773d014014f9fbb5df0e98d.png" width="425" height="531" class="img_ev3q"></p><p>출처: <a href="https://www.nature.com/articles/nature16961" target="_blank" rel="noopener noreferrer">Mastering the game of Go with deep neural networks and tree search</a></p><p>여기서 판단하는 방식이 2가지가 있는데, 첫 번째로는 Value Network 를 써서 바둑판의 상태를 보고 승률을 예측해봅니다.
두 번째로 위에서 언급했던 SL Policy Network 보다 가볍고 빠른 Rollout Policy Network 을 이용해서 가상으로 끝까지 게임을 플레이 합니다.
예를 들면 끝까지 게임을 두어서 내가 흑돌로 플레이 한 상태로 흑돌이 이겼으면 +1 졌으면 -1 보상을 받습니다.
Value Network 의 값과 Rollout Policy Network 로 플레이한 게임의 결과를 합하여 그 수가 얼마나 좋은지 판단합니다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="backup">Backup<a href="#backup" class="hash-link" aria-label="Direct link to Backup" title="Direct link to Backup">​</a></h2><p>이렇게 Selection, Expansion, Evaluation 을 해서 얻은 값을 지나온 경로의 상태들에 반영됩니다.
반영된 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em"></span><span class="mord mathnormal">Q</span></span></span></span></span> 값을 보고 다음 Selection, Expansion, Evaluation 을 반복합니다.</p><p><img loading="lazy" alt="MCTS Backup" src="/assets/images/mcts-step-backup-cbb46c7fdee13ed0f47acb1f3a005e85.png" width="513" height="546" class="img_ev3q"></p><p>출처: <a href="https://www.nature.com/articles/nature16961" target="_blank" rel="noopener noreferrer">Mastering the game of Go with deep neural networks and tree search</a></p><h1>Conclusion</h1><p>바둑에서는 셀 수 없이 많은 경우의 수가 존재하지만 AlphaGo 는 탐색할 경우의 수를 줄이면서 프로 바둑 기사에게 승리를 거둡니다.
수 읽기 시간에 많은 시뮬레이션을 통해 가능한 경우의 수들을 트리로 구성합니다.
트리를 구성하는 과정에서 전문가들의 기보를 학습한 Policy Network 로 가능성이 있는 수들만 탐색하고,
Value Network 와 Rollout Policy Network 을 이용하여 승률을 예측합니다.
많은 수를 경험하면 트리의 정확도는 올라가지만 바둑에서는 시간이 제한적이기 때문에 탐색 수를 적절하게 조절해야 합니다.
Atari 게임에서 강화학습을 이용한 사례들과는 다르게 사람의 기보를 기반으로 만든 Network 를 사용한다는 한계가 있습니다.</p></div><footer class="row docusaurus-mt-lg blogPostFooterDetailsFull_mRVl"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/reinforcement-learning/">reinforcement learning</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/alphago/">alphago</a></li></ul></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/blog/2019/02/02/alphago-zero/"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">AlphaGo Paper Review (2)</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#가능성이-있는-수들만-탐색해본다" class="table-of-contents__link toc-highlight">가능성이 있는 수들만 탐색해본다.</a></li><li><a href="#승산이-없는-게임은-굳이-더-탐색하지-않는다" class="table-of-contents__link toc-highlight">승산이 없는 게임은 굳이 더 탐색하지 않는다.</a></li><li><a href="#sl-policy-network" class="table-of-contents__link toc-highlight">SL Policy Network</a></li><li><a href="#rollout-policy-network" class="table-of-contents__link toc-highlight">Rollout Policy Network</a></li><li><a href="#rl-policy-network" class="table-of-contents__link toc-highlight">RL Policy Network</a></li><li><a href="#value-network" class="table-of-contents__link toc-highlight">Value Network</a></li><li><a href="#selection-expansion" class="table-of-contents__link toc-highlight">Selection, Expansion</a></li><li><a href="#evaluation" class="table-of-contents__link toc-highlight">Evaluation</a></li><li><a href="#backup" class="table-of-contents__link toc-highlight">Backup</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 yongseongkim</div></div></div></footer></div>
<script src="/assets/js/runtime~main.c5770a4c.js"></script>
<script src="/assets/js/main.f9b9ab4d.js"></script>
</body>
</html>